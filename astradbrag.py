# -*- coding: utf-8 -*-
"""AstraDBRAG

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16wcZdOWvX_mJbU8jAe4Fm4iIHigtkHBn
"""

# pip install langchain_astradb
# pip install transformers
# pip install langchain_openai
# pip install langchain_core
# pip install datasets

import os
from getpass import getpass

os.environ["ASTRA_DB_APPLICATION_TOKEN"] = getpass("ASTRA_DB_APPLICATION_TOKEN = ")
os.environ["ASTRA_DB_API_ENDPOINT"] = input("ASTRA_DB_API_ENDPOINT = ")

os.environ["OPENAI_API_KEY"] = getpass("OPENAI_API_KEY = ")

from langchain_astradb import AstraDBVectorStore
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings

from datasets import load_dataset

ASTRA_DB_APPLICATION_TOKEN = os.environ.get("ASTRA_DB_APPLICATION_TOKEN")
ASTRA_DB_API_ENDPOINT = os.environ.get("ASTRA_DB_API_ENDPOINT")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

embedding = OpenAIEmbeddings()
vstore = AstraDBVectorStore(
    embedding=embedding,
    collection_name="orca2000",
    token=os.environ["ASTRA_DB_APPLICATION_TOKEN"],
    api_endpoint=os.environ["ASTRA_DB_API_ENDPOINT"],
)

dataset = load_dataset("microsoft/orca-math-word-problems-200k", split='train')

docs = []
for entry in dataset:
    # Add a LangChain document with the answer and metadata tags
    content = f"Question: {entry['question']}\nAnswer: {entry['answer']}"
    doc = Document(page_content=content, metadata={"answer": entry["answer"]})
    docs.append(doc)

docs = docs[:2000]

inserted_ids = vstore.add_documents(docs)
print(f"\nInserted {len(inserted_ids)} documents.")

def retrieve_documents(query):
    results = vstore.similarity_search(query, k=1)
    return results

results = vstore.similarity_search("Jungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.", k=3)
for res in results:
    print(f"* {res.page_content} + {res.metadata}")

from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf")
model.cuda()

def generate_prompt(content, query):
    B_INST, E_INST = "[INST]", "[/INST]"
    B_SYS, E_SYS = "<<SYS>>\n", "\n<</SYS>>\n\n"
    SYS_PROMPT = B_SYS + "Use the following context to help answer the user question. Only provide the answer, nothing more." + E_SYS

    instruction = f"Context: {content} User Question: {query}"

    prompt_template = B_INST + SYS_PROMPT + instruction + E_INST
    return prompt_template



def chat_with_llama(content, query):

    def cut_off_text(text, prompt):
        cutoff_phrase = prompt
        index = text.find(cutoff_phrase)
        if index != -1:
          return text[:index]
        else:
          return text

    def remove_substring(string, substring):
        return string.replace(substring, "")

    prompt = generate_prompt(content, query)

    input_ids = tokenizer.encode(prompt, return_tensors="pt").to('cuda')
    output = model.generate(input_ids)
    response = tokenizer.batch_decode(output, skip_special_tokens=True)
    response = cut_off_text(response[0], '</s>')
    response = remove_substring(response, prompt)
    return response

def interactive_chat():
    print("LLaMA 2 Chatbot: I'm here to help with math problems. Ask me anything!")
    while True:
        user_input = input("You: ")
        if user_input.lower() == 'exit':
            print("LLaMA 2 Chatbot: Goodbye!")
            break
        documents = retrieve_documents(user_input)
        context = " ".join([doc.page_content for doc in documents])
        response = chat_with_llama(context, user_input)
        print("LLaMA 2 Chatbot:", response)



if __name__ == '__main__':
    interactive_chat()

